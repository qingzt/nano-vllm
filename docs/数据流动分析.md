# Nano-vLLM æ•°æ®æµåŠ¨åˆ†ææ–‡æ¡£

## ç›®å½•
1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
3. [è¯¦ç»†æ•°æ®æµåŠ¨ç¤ºä¾‹](#è¯¦ç»†æ•°æ®æµåŠ¨ç¤ºä¾‹)
4. [å…³é”®ç»„ä»¶è¯´æ˜](#å…³é”®ç»„ä»¶è¯´æ˜)
5. [ä¼˜åŒ–æœºåˆ¶](#ä¼˜åŒ–æœºåˆ¶)

---

## é¡¹ç›®æ¦‚è¿°

Nano-vLLM æ˜¯ä¸€ä¸ªä»é›¶å®ç°çš„è½»é‡çº§ vLLM æ¨ç†å¼•æ“ï¼Œä½¿ç”¨çº¦ 1200 è¡Œ Python ä»£ç å®ç°äº†é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹ç¦»çº¿æ¨ç†ã€‚é¡¹ç›®é‡‡ç”¨äº† PagedAttentionã€Prefix Cachingã€Tensor Parallelismã€CUDA Graph ç­‰å¤šç§ä¼˜åŒ–æŠ€æœ¯ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸš€ æ¥è¿‘ vLLM çš„æ¨ç†é€Ÿåº¦
- ğŸ“– ç®€æ´æ˜“è¯»çš„ä»£ç å®ç°
- âš¡ å®Œæ•´çš„ä¼˜åŒ–å¥—ä»¶ï¼ˆPrefix Cachingã€Tensor Parallelismã€CUDA Graph ç­‰ï¼‰

---

## æ ¸å¿ƒæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç”¨æˆ·å±‚ (User)                          â”‚
â”‚                     LLM.generate()                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¼•æ“å±‚ (LLMEngine)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Scheduler   â”‚  â”‚ BlockManager â”‚  â”‚  ModelRunner    â”‚   â”‚
â”‚  â”‚  (è°ƒåº¦å™¨)    â”‚  â”‚  (å—ç®¡ç†å™¨)  â”‚  â”‚  (æ¨¡å‹æ‰§è¡Œå™¨)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ¨¡å‹å±‚ (Model Layer)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Qwen3      â”‚  â”‚  Attention   â”‚  â”‚     Sampler     â”‚   â”‚
â”‚  â”‚ CausalLM     â”‚  â”‚   (æ³¨æ„åŠ›)   â”‚  â”‚    (é‡‡æ ·å™¨)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## è¯¦ç»†æ•°æ®æµåŠ¨ç¤ºä¾‹

### ç¤ºä¾‹åœºæ™¯
å‡è®¾ç”¨æˆ·è¾“å…¥ä¸¤ä¸ª promptï¼š
- Prompt 1: "introduce yourself" (ç¼–ç å 20 ä¸ª token)
- Prompt 2: "list all prime numbers within 100" (ç¼–ç å 30 ä¸ª token)

é‡‡æ ·å‚æ•°ï¼š`temperature=0.6, max_tokens=256`

### æ­¥éª¤ 1: åˆå§‹åŒ–é˜¶æ®µ

```python
# ç”¨æˆ·ä»£ç 
llm = LLM("models/Qwen3-0.6B/", enforce_eager=True, tensor_parallel_size=1)
```

**æ•°æ®æµåŠ¨ï¼š**

1. **Config åˆå§‹åŒ–** ([config.py](config.py))
   ```
   è¾“å…¥å‚æ•° â†’ Config å¯¹è±¡
   - model: "models/Qwen3-0.6B/"
   - max_num_batched_tokens: 16384
   - max_num_seqs: 512
   - max_model_len: 4096
   - gpu_memory_utilization: 0.8
   - kvcache_block_size: 256
   ```

2. **ModelRunner åˆå§‹åŒ–** ([model_runner.py](model_runner.py#L18))
   ```
   Config â†’ ModelRunner
   â”œâ”€ åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ (NCCL/GLOO)
   â”œâ”€ åŠ è½½ Qwen3ForCausalLM æ¨¡å‹
   â”œâ”€ åˆ›å»º Sampler é‡‡æ ·å™¨
   â”œâ”€ warmup_model(): é¢„çƒ­æ¨¡å‹
   â”‚  â””â”€ æ„é€ å‡åºåˆ— â†’ æ‰§è¡Œä¸€æ¬¡ prefill â†’ æ¸…ç©ºç¼“å­˜
   â””â”€ allocate_kv_cache(): åˆ†é… KV ç¼“å­˜
      â”œâ”€ è®¡ç®—å¯ç”¨æ˜¾å­˜
      â”œâ”€ ç¡®å®šç¼“å­˜å—æ•°é‡: num_kvcache_blocks
      â””â”€ åˆ›å»º kv_cache å¼ é‡ [2, num_layers, num_blocks, block_size, num_kv_heads, head_dim]
   ```

3. **Scheduler åˆå§‹åŒ–** ([scheduler.py](scheduler.py#L9))
   ```
   Config â†’ Scheduler
   â”œâ”€ BlockManager: ç®¡ç† KV ç¼“å­˜å—åˆ†é…
   â”œâ”€ waiting: ç­‰å¾…é˜Ÿåˆ— (deque)
   â””â”€ running: è¿è¡Œé˜Ÿåˆ— (deque)
   ```

### æ­¥éª¤ 2: æ·»åŠ è¯·æ±‚

```python
prompts = ["introduce yourself", "list all prime numbers within 100"]
outputs = llm.generate(prompts, sampling_params)
```

**æ•°æ®æµåŠ¨ï¼š**

1. **Token ç¼–ç ** ([llm_engine.py](llm_engine.py#L48))
   ```
   Prompt å­—ç¬¦ä¸² â†’ Tokenizer.encode()
   "introduce yourself" â†’ [101, 4351, 1045, ..., 102]  (20 tokens)
   "list all primes..." â†’ [101, 1231, 1092, ..., 102]  (30 tokens)
   ```

2. **åˆ›å»º Sequence å¯¹è±¡** ([sequence.py](sequence.py#L18))
   ```
   Token IDs + SamplingParams â†’ Sequence å¯¹è±¡
   
   Sequence 1:
   â”œâ”€ seq_id: 0
   â”œâ”€ status: WAITING
   â”œâ”€ token_ids: [101, 4351, ..., 102]  (20 tokens)
   â”œâ”€ num_tokens: 20
   â”œâ”€ num_prompt_tokens: 20
   â”œâ”€ num_cached_tokens: 0
   â”œâ”€ block_table: []
   â”œâ”€ temperature: 0.6
   â””â”€ max_tokens: 256
   
   Sequence 2:
   â”œâ”€ seq_id: 1
   â”œâ”€ status: WAITING
   â”œâ”€ token_ids: [101, 1231, ..., 102]  (30 tokens)
   â”œâ”€ num_tokens: 30
   â”œâ”€ ...
   ```

3. **åŠ å…¥è°ƒåº¦é˜Ÿåˆ—** ([scheduler.py](scheduler.py#L17))
   ```
   Sequence â†’ Scheduler.waiting é˜Ÿåˆ—
   waiting: [Seq(0), Seq(1)]
   running: []
   ```

### æ­¥éª¤ 3: Prefill é˜¶æ®µï¼ˆç¬¬ 1 æ¬¡ stepï¼‰

**æ•°æ®æµåŠ¨ï¼š**

1. **è°ƒåº¦å†³ç­–** ([scheduler.py](scheduler.py#L19))
   ```
   Scheduler.schedule() æ‰§è¡Œï¼š
   
   æ£€æŸ¥ waiting é˜Ÿåˆ—:
   â”œâ”€ Seq(0): 20 tokens
   â”‚  â”œâ”€ num_batched_tokens: 0 + 20 = 20 â‰¤ 16384 âœ“
   â”‚  â”œâ”€ BlockManager.can_allocate(): éœ€è¦ 1 ä¸ª block âœ“
   â”‚  â””â”€ çŠ¶æ€: WAITING â†’ RUNNING
   â”‚
   â”œâ”€ Seq(1): 30 tokens
   â”‚  â”œâ”€ num_batched_tokens: 20 + 30 = 50 â‰¤ 16384 âœ“
   â”‚  â”œâ”€ BlockManager.can_allocate(): éœ€è¦ 1 ä¸ª block âœ“
   â”‚  â””â”€ çŠ¶æ€: WAITING â†’ RUNNING
   
   ç»“æœ:
   scheduled_seqs: [Seq(0), Seq(1)]
   is_prefill: True
   waiting: []
   running: [Seq(0), Seq(1)]
   ```

2. **Block åˆ†é…** ([block_manager.py](block_manager.py#L47))
   ```
   BlockManager.allocate() æ‰§è¡Œï¼š
   
   Seq(0) åˆ†é…:
   â”œâ”€ token_ids: [101, 4351, ..., 102]  (20 tokens)
   â”œâ”€ num_blocks: (20 + 256 - 1) // 256 = 1
   â”œâ”€ block(0): [101, 4351, ..., 102, 0, 0, ...] (256 slots, åªç”¨ 20)
   â”œâ”€ è®¡ç®— hash: xxhash(block(0))
   â”œâ”€ æ£€æŸ¥ Prefix Cache: cache miss
   â”œâ”€ åˆ†é… block_id: 0
   â”œâ”€ block_table: [0]
   â””â”€ num_cached_tokens: 0 (é¦–æ¬¡æ¨ç†æ— ç¼“å­˜)
   
   Seq(1) åˆ†é…:
   â”œâ”€ token_ids: [101, 1231, ..., 102]  (30 tokens)
   â”œâ”€ num_blocks: (30 + 256 - 1) // 256 = 1
   â”œâ”€ åˆ†é… block_id: 1
   â”œâ”€ block_table: [1]
   â””â”€ num_cached_tokens: 0
   ```

3. **å‡†å¤‡ Prefill è¾“å…¥** ([model_runner.py](model_runner.py#L124))
   ```
   ModelRunner.prepare_prefill() æ„é€ ï¼š
   
   input_ids: æ‰€æœ‰æœªç¼“å­˜çš„ token
   [101, 4351, ..., 102,           # Seq(0) çš„ 20 ä¸ª token
    101, 1231, ..., 102]           # Seq(1) çš„ 30 ä¸ª token
   â†’ Tensor: shape [50]
   
   positions: æ¯ä¸ª token çš„ä½ç½®ç¼–å·
   [0, 1, 2, ..., 19,              # Seq(0)
    0, 1, 2, ..., 29]              # Seq(1)
   â†’ Tensor: shape [50]
   
   cu_seqlens_q: query åºåˆ—é•¿åº¦ç´¯åŠ å’Œ (ç”¨äº FlashAttention)
   [0, 20, 50]
   
   cu_seqlens_k: key åºåˆ—é•¿åº¦ç´¯åŠ å’Œ
   [0, 20, 50]
   
   max_seqlen_q: 30
   max_seqlen_k: 30
   
   slot_mapping: æ¯ä¸ª token åœ¨ KV cache ä¸­çš„ç‰©ç†ä½ç½®
   [0, 1, 2, ..., 19,              # Seq(0) â†’ Block 0 çš„ slot 0-19
    256, 257, ..., 285]            # Seq(1) â†’ Block 1 çš„ slot 256-285
   â†’ Tensor: shape [50]
   ```

4. **æ¨¡å‹å‰å‘ä¼ æ’­** ([model_runner.py](model_runner.py#L187), [qwen3.py](qwen3.py))
   ```
   input_ids [50] â†’ Embedding Layer
   â†“
   hidden_states [50, 4096]
   â†“
   for layer in 28 layers:
       â”œâ”€ RMSNorm
       â”œâ”€ Attention:
       â”‚  â”œâ”€ QKV Linear: [50, 4096] â†’ Q[50, 16, 256], K[50, 2, 256], V[50, 2, 256]
       â”‚  â”œâ”€ RoPE: æ—‹è½¬ä½ç½®ç¼–ç 
       â”‚  â”œâ”€ Flash Attention (varlen):
       â”‚  â”‚  â”œâ”€ Q: [50, 16, 256]
       â”‚  â”‚  â”œâ”€ K: [50, 2, 256] â†’ å†™å…¥ k_cache[block_ids, slots]
       â”‚  â”‚  â”œâ”€ V: [50, 2, 256] â†’ å†™å…¥ v_cache[block_ids, slots]
       â”‚  â”‚  â””â”€ Output: [50, 16, 256]
       â”‚  â””â”€ O Linear: [50, 4096]
       â”œâ”€ RMSNorm
       â””â”€ MLP: [50, 4096] â†’ [50, 11008] â†’ [50, 4096]
   â†“
   hidden_states [50, 4096]
   â†“
   LM Head: [50, 4096] â†’ logits [50, 151936]
   ```

5. **é‡‡æ ·** ([sampler.py](sampler.py#L8))
   ```
   logits [50, 151936] + temperatures [2]
   â†“
   logits / temperature (broadcast)
   â”œâ”€ Seq(0) last token (index 19): logits[19] / 0.6
   â””â”€ Seq(1) last token (index 49): logits[49] / 0.6
   â†“
   softmax â†’ probs [50, 151936]
   â†“
   Gumbel sampling:
   probs / exponential(1) â†’ argmax
   â†“
   sample_tokens:
   â”œâ”€ Seq(0): token_id = 3456  (ä¾‹å¦‚ "I")
   â””â”€ Seq(1): token_id = 7890  (ä¾‹å¦‚ "Sure")
   ```

6. **åå¤„ç†** ([scheduler.py](scheduler.py#L57))
   ```
   Scheduler.postprocess():
   
   Seq(0).append_token(3456):
   â”œâ”€ token_ids: [..., 102, 3456]
   â”œâ”€ num_tokens: 20 â†’ 21
   â”œâ”€ num_completion_tokens: 0 â†’ 1
   â”œâ”€ last_token: 3456
   â””â”€ status: RUNNING (æœªè¾¾åˆ° max_tokens æˆ– EOS)
   
   Seq(1).append_token(7890):
   â”œâ”€ token_ids: [..., 102, 7890]
   â”œâ”€ num_tokens: 30 â†’ 31
   â””â”€ status: RUNNING
   
   ç»“æœ:
   outputs: []  (æ— åºåˆ—å®Œæˆ)
   running: [Seq(0), Seq(1)]
   ```

### æ­¥éª¤ 4: Decode é˜¶æ®µï¼ˆç¬¬ 2-N æ¬¡ stepï¼‰

**æ•°æ®æµåŠ¨ï¼š**

1. **è°ƒåº¦å†³ç­–** ([scheduler.py](scheduler.py#L35))
   ```
   Scheduler.schedule() æ‰§è¡Œï¼š
   
   æ£€æŸ¥ running é˜Ÿåˆ—:
   â”œâ”€ Seq(0): 21 tokens, éœ€è¦ append 1 ä¸ª token
   â”‚  â”œâ”€ BlockManager.can_append(): 
   â”‚  â”‚  â””â”€ 21 % 256 = 21 â‰  1, å½“å‰ block æœªæ»¡, ä¸éœ€è¦æ–° block âœ“
   â”‚  â””â”€ may_append(): æ ‡è®°å°†ä½¿ç”¨ slot
   â”‚
   â”œâ”€ Seq(1): 31 tokens
   â”‚  â””â”€ åŒç†
   
   ç»“æœ:
   scheduled_seqs: [Seq(0), Seq(1)]
   is_prefill: False
   ```

2. **å‡†å¤‡ Decode è¾“å…¥** ([model_runner.py](model_runner.py#L163))
   ```
   ModelRunner.prepare_decode() æ„é€ ï¼š
   
   input_ids: æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª token
   [3456,                          # Seq(0) æ–°ç”Ÿæˆçš„ token
    7890]                          # Seq(1) æ–°ç”Ÿæˆçš„ token
   â†’ Tensor: shape [2]
   
   positions: å½“å‰ç”Ÿæˆä½ç½®
   [20,                            # Seq(0) çš„ç¬¬ 21 ä¸ªä½ç½®
    30]                            # Seq(1) çš„ç¬¬ 31 ä¸ªä½ç½®
   â†’ Tensor: shape [2]
   
   context_lens: å®Œæ•´ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆç”¨äº Paged Attentionï¼‰
   [21, 31]
   â†’ Tensor: shape [2]
   
   slot_mapping: æ–° token å†™å…¥ KV cache çš„ä½ç½®
   [20,                            # Seq(0) â†’ Block 0 çš„ slot 20
    286]                           # Seq(1) â†’ Block 1 çš„ slot 286
   â†’ Tensor: shape [2]
   
   block_tables: æ¯ä¸ªåºåˆ—çš„ block æ˜ å°„è¡¨ï¼ˆè¡¥é½åˆ°ç›¸åŒé•¿åº¦ï¼‰
   [[0, -1, -1, ...],              # Seq(0) åªç”¨ 1 ä¸ª block
    [1, -1, -1, ...]]              # Seq(1) åªç”¨ 1 ä¸ª block
   â†’ Tensor: shape [2, max_blocks]
   ```

3. **æ¨¡å‹å‰å‘ä¼ æ’­** ([model_runner.py](model_runner.py#L187))
   ```
   input_ids [2] â†’ Embedding Layer
   â†“
   hidden_states [2, 4096]
   â†“
   for layer in 28 layers:
       â”œâ”€ RMSNorm
       â”œâ”€ Attention:
       â”‚  â”œâ”€ QKV Linear: [2, 4096] â†’ Q[2, 16, 256], K[2, 2, 256], V[2, 2, 256]
       â”‚  â”œâ”€ RoPE: æ—‹è½¬ä½ç½®ç¼–ç 
       â”‚  â”œâ”€ Paged Attention:
       â”‚  â”‚  â”œâ”€ Q: [2, 16, 256]  (æ–° token çš„ query)
       â”‚  â”‚  â”œâ”€ ä» k_cache è¯»å–å†å² K:
       â”‚  â”‚  â”‚  â”œâ”€ Seq(0): k_cache[0, 0:21] â†’ [21, 2, 256]
       â”‚  â”‚  â”‚  â””â”€ Seq(1): k_cache[1, 0:31] â†’ [31, 2, 256]
       â”‚  â”‚  â”œâ”€ ä» v_cache è¯»å–å†å² V
       â”‚  â”‚  â”œâ”€ æ–° K, V å†™å…¥å¯¹åº” slot
       â”‚  â”‚  â””â”€ Attention(Q, K_all, V_all) â†’ [2, 16, 256]
       â”‚  â””â”€ O Linear: [2, 4096]
       â”œâ”€ RMSNorm
       â””â”€ MLP: [2, 4096] â†’ [2, 11008] â†’ [2, 4096]
   â†“
   hidden_states [2, 4096]
   â†“
   LM Head: [2, 4096] â†’ logits [2, 151936]
   ```

4. **é‡‡æ ·ä¸åå¤„ç†**
   ```
   logits [2, 151936] â†’ Sampler
   â†“
   new_tokens:
   â”œâ”€ Seq(0): 1234  (ä¾‹å¦‚ "am")
   â””â”€ Seq(1): 5678  (ä¾‹å¦‚ ",")
   
   Postprocess:
   â”œâ”€ Seq(0): num_tokens: 21 â†’ 22, status: RUNNING
   â””â”€ Seq(1): num_tokens: 31 â†’ 32, status: RUNNING
   ```

5. **Block æ‰©å±•ï¼ˆå½“éœ€è¦æ—¶ï¼‰** ([block_manager.py](block_manager.py#L95))
   ```
   å‡è®¾ Seq(0) ç”Ÿæˆåˆ° 256 tokens:
   
   len(seq) = 256, 256 % 256 = 0
   â”œâ”€ ä¸Šä¸€ä¸ª block å·²æ»¡
   â”œâ”€ è®¡ç®— hash: xxhash(block(0)) â†’ å­˜å‚¨åˆ° hash_to_block_id
   â””â”€ ä¸‹æ¬¡éœ€è¦æ–° block æ—¶ä¼šæ£€æŸ¥ Prefix Cache
   
   len(seq) = 257, 257 % 256 = 1
   â”œâ”€ éœ€è¦æ–° block
   â”œâ”€ åˆ†é… block_id: 2
   â”œâ”€ block_table: [0, 2]
   â””â”€ slot_mapping: 256 å†™åˆ° block 0 æœ€åï¼Œ257 å†™åˆ° block 2 å¼€å¤´
   ```

### æ­¥éª¤ 5: å®Œæˆé˜¶æ®µ

```
å¾ªç¯æ‰§è¡Œ Decode ç›´åˆ°:

Seq(0): ç”Ÿæˆ 256 tokens æˆ–é‡åˆ° EOS
â”œâ”€ status: RUNNING â†’ FINISHED
â”œâ”€ BlockManager.deallocate(): é‡Šæ”¾ blocks [0]
â”œâ”€ ä» running é˜Ÿåˆ—ç§»é™¤
â””â”€ outputs è¿”å›: (seq_id=0, completion_tokens=[3456, 1234, ...])

Seq(1): åŒç†
â””â”€ outputs è¿”å›: (seq_id=1, completion_tokens=[7890, 5678, ...])

æ‰€æœ‰åºåˆ—å®Œæˆå:
â”œâ”€ Token è§£ç : Tokenizer.decode(token_ids) â†’ æ–‡æœ¬
â””â”€ è¿”å›ç»™ç”¨æˆ·: [{"text": "I am...", "token_ids": [...]}, ...]
```

---

## å…³é”®ç»„ä»¶è¯´æ˜

### 1. Sequenceï¼ˆåºåˆ—ï¼‰

**æ ¸å¿ƒæ•°æ®ç»“æ„** ([sequence.py](sequence.py))

```python
class Sequence:
    seq_id: int                    # å”¯ä¸€åºåˆ— ID
    status: SequenceStatus         # WAITING / RUNNING / FINISHED
    token_ids: list[int]           # å®Œæ•´ token åºåˆ—
    num_tokens: int                # æ€» token æ•°
    num_prompt_tokens: int         # prompt éƒ¨åˆ† token æ•°
    num_cached_tokens: int         # å·²ç¼“å­˜çš„ token æ•°
    block_table: list[int]         # åˆ†é…çš„ block ID åˆ—è¡¨
    temperature: float             # é‡‡æ ·æ¸©åº¦
    max_tokens: int                # æœ€å¤§ç”Ÿæˆ token æ•°
```

**å…³é”®å±æ€§ï¼š**
- `num_blocks`: éœ€è¦çš„ block æ•°é‡ = âŒˆnum_tokens / block_sizeâŒ‰
- `last_block_num_tokens`: æœ€åä¸€ä¸ª block ä¸­çš„ token æ•°
- `num_cached_blocks`: å·²ç¼“å­˜çš„å®Œæ•´ block æ•°

### 2. BlockManagerï¼ˆå—ç®¡ç†å™¨ï¼‰

**åŠŸèƒ½** ([block_manager.py](block_manager.py))

ç®¡ç† KV Cache çš„ç‰©ç†å†…å­˜å—ï¼Œå®ç° PagedAttention å’Œ Prefix Cachingã€‚

```
KV Cache å¸ƒå±€:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Block 0   â”‚  Block 1   â”‚  Block 2   â”‚ ... â”‚  Block N  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 256 slots  â”‚ 256 slots  â”‚ 256 slots  â”‚ ... â”‚ 256 slots â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘              â†‘
     â”‚              â”‚
Seq A çš„          Seq B çš„
block_table:      block_table:
[0, 2, 5]         [1, 3]
```

**å…³é”®æ–¹æ³•ï¼š**

- `can_allocate(seq)`: æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—² block
- `allocate(seq)`: ä¸ºåºåˆ—åˆ†é… blocksï¼Œæ£€æŸ¥ Prefix Cache
- `deallocate(seq)`: é‡Šæ”¾åºåˆ—çš„ blocksï¼ˆå¼•ç”¨è®¡æ•°ï¼‰
- `can_append(seq)`: æ£€æŸ¥æ˜¯å¦éœ€è¦æ–° block
- `may_append(seq)`: æ ‡è®°å°†è¿½åŠ æ–° token

**Prefix Caching æœºåˆ¶ï¼š**

```python
# è®¡ç®— block çš„ hash
hash = xxhash(block_tokens, prefix_hash)

# æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒå‰ç¼€çš„ç¼“å­˜ block
if hash in hash_to_block_id:
    # å¤ç”¨å·²æœ‰ blockï¼Œå¢åŠ å¼•ç”¨è®¡æ•°
    block.ref_count += 1
    seq.num_cached_tokens += block_size
else:
    # åˆ†é…æ–° block
    allocate_new_block()
```

### 3. Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰

**åŠŸèƒ½** ([scheduler.py](scheduler.py))

å†³å®šæ¯ä¸ª step æ‰§è¡Œå“ªäº›åºåˆ—ï¼Œåè°ƒ Prefill å’Œ Decodeã€‚

**è°ƒåº¦ç­–ç•¥ï¼š**

1. **Prefill ä¼˜å…ˆ**ï¼šä¼˜å…ˆå¤„ç†æ–°è¯·æ±‚
   - çº¦æŸï¼š`num_batched_tokens â‰¤ max_num_batched_tokens`
   - çº¦æŸï¼š`num_seqs â‰¤ max_num_seqs`
   - çº¦æŸï¼šæœ‰è¶³å¤Ÿçš„ KV Cache blocks

2. **Decode æ‰¹å¤„ç†**ï¼šåŒæ—¶ç”Ÿæˆå¤šä¸ªåºåˆ—çš„ä¸‹ä¸€ä¸ª token
   - çº¦æŸï¼š`num_seqs â‰¤ max_num_seqs`
   - çº¦æŸï¼šæœ‰è¶³å¤Ÿçš„ KV Cache blocks

3. **Preemptionï¼ˆæŠ¢å ï¼‰**ï¼šKV Cache ä¸è¶³æ—¶
   - å°†æ­£åœ¨è¿è¡Œçš„åºåˆ—ç§»å›ç­‰å¾…é˜Ÿåˆ—
   - é‡Šæ”¾å…¶ KV Cache blocks
   - ä¸‹æ¬¡é‡æ–°è°ƒåº¦æ—¶éœ€è¦é‡æ–° prefill

### 4. ModelRunnerï¼ˆæ¨¡å‹æ‰§è¡Œå™¨ï¼‰

**åŠŸèƒ½** ([model_runner.py](model_runner.py))

æ‰§è¡Œæ¨¡å‹æ¨ç†ï¼Œç®¡ç† GPU èµ„æºå’Œ CUDA Graphã€‚

**å…³é”®æ–¹æ³•ï¼š**

- `warmup_model()`: é¢„çƒ­æ¨¡å‹ï¼Œç¡®å®šå†…å­˜å ç”¨
- `allocate_kv_cache()`: æ ¹æ®å¯ç”¨æ˜¾å­˜åˆ†é… KV Cache
- `prepare_prefill()`: æ„é€  Prefill è¾“å…¥ï¼ˆFlashAttention varlen æ ¼å¼ï¼‰
- `prepare_decode()`: æ„é€  Decode è¾“å…¥ï¼ˆPaged Attention æ ¼å¼ï¼‰
- `run_model()`: æ‰§è¡Œæ¨ç†ï¼ˆeager æˆ– CUDA graphï¼‰
- `capture_cudagraph()`: æ•è·ä¸åŒ batch size çš„ CUDA graph

**CUDA Graph ä¼˜åŒ–ï¼š**

```python
# é¢„å…ˆæ•è·ä¸åŒ batch size çš„è®¡ç®—å›¾
graph_bs = [1, 2, 4, 8, 16, 32, ..., 512]

for bs in graph_bs:
    # æ•è·è®¡ç®—å›¾
    with torch.cuda.graph(graph):
        outputs = model(input_ids[:bs], positions[:bs])
    graphs[bs] = graph

# æ¨ç†æ—¶å¤ç”¨è®¡ç®—å›¾ï¼ˆé¿å… kernel launch overheadï¼‰
graph = graphs[next(x for x in graph_bs if x >= current_bs)]
graph.replay()
```

### 5. Attentionï¼ˆæ³¨æ„åŠ›å±‚ï¼‰

**åŠŸèƒ½** ([attention.py](attention.py))

å®ç°é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæ”¯æŒ Prefill å’Œ Decode ä¸¤ç§æ¨¡å¼ã€‚

**Prefill æ¨¡å¼**ï¼ˆFlashAttention varlenï¼‰ï¼š

```python
# ä¸åŒé•¿åº¦çš„åºåˆ—æ‹¼æ¥æˆä¸€ä¸ªå¤§å¼ é‡
Q: [total_tokens, num_heads, head_dim]
K: [total_tokens, num_kv_heads, head_dim]
V: [total_tokens, num_kv_heads, head_dim]

# cu_seqlens è®°å½•æ¯ä¸ªåºåˆ—çš„è¾¹ç•Œ
cu_seqlens_q: [0, len1, len1+len2, ...]
cu_seqlens_k: [0, len1, len1+len2, ...]

# FlashAttention è‡ªåŠ¨å¤„ç†å˜é•¿åºåˆ—
output = flash_attn_varlen_func(Q, K, V, cu_seqlens_q, cu_seqlens_k, ...)
```

**Decode æ¨¡å¼**ï¼ˆPaged Attentionï¼‰ï¼š

```python
# Q: æ¯ä¸ªåºåˆ—åªæœ‰ä¸€ä¸ªæ–° token
Q: [batch_size, num_heads, head_dim]

# K, V: ä» KV Cache ä¸­è¯»å–æ‰€æœ‰å†å² tokens
for seq_idx in range(batch_size):
    block_ids = block_tables[seq_idx]
    context_len = context_lens[seq_idx]
    
    # ä»å¤šä¸ª blocks ä¸­æ‹¼æ¥ K, V
    K_seq = k_cache[block_ids, :context_len]
    V_seq = v_cache[block_ids, :context_len]
    
    # æ³¨æ„åŠ›è®¡ç®—
    output[seq_idx] = attention(Q[seq_idx], K_seq, V_seq)
```

### 6. Samplerï¼ˆé‡‡æ ·å™¨ï¼‰

**åŠŸèƒ½** ([sampler.py](sampler.py))

ä» logits ä¸­é‡‡æ ·ä¸‹ä¸€ä¸ª tokenã€‚

**Gumbel Sampling å®ç°ï¼š**

```python
# æ¸©åº¦ç¼©æ”¾
logits = logits / temperature

# Softmax å½’ä¸€åŒ–
probs = softmax(logits)

# Gumbel é‡‡æ ·ï¼šprobs / Exp(1) ç„¶åå– argmax
# ç­‰ä»·äºä» Multinomial åˆ†å¸ƒé‡‡æ ·ï¼Œä½†æ›´é«˜æ•ˆ
noise = exponential(1).clamp_min(1e-10)
sample_tokens = argmax(probs / noise)
```

---

## ä¼˜åŒ–æœºåˆ¶

### 1. PagedAttention

**åŸç†ï¼š** å°† KV Cache åˆ†å—ç®¡ç†ï¼Œç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜åˆ†é¡µã€‚

**ä¼˜åŠ¿ï¼š**
- å‡å°‘å†…å­˜ç¢ç‰‡
- æ”¯æŒåŠ¨æ€å†…å­˜åˆ†é…
- æé«˜ GPU æ˜¾å­˜åˆ©ç”¨ç‡

**ç¤ºä¾‹ï¼š**

```
ä¼ ç»Ÿ KV Cacheï¼ˆè¿ç»­å­˜å‚¨ï¼‰:
Seq A: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        ]  æµªè´¹ç©ºé—´
Seq B: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    ]  æµªè´¹ç©ºé—´

PagedAttentionï¼ˆå—å­˜å‚¨ï¼‰:
Seq A: Block 0 [â–ˆâ–ˆâ–ˆâ–ˆ] â†’ Block 2 [â–ˆâ–ˆâ–ˆâ–ˆ] â†’ Block 5 [â–ˆâ–ˆâ–ˆâ–ˆ]
Seq B: Block 1 [â–ˆâ–ˆâ–ˆâ–ˆ] â†’ Block 3 [â–ˆâ–ˆâ–ˆ ]
```

### 2. Prefix Caching

**åŸç†ï¼š** ç¼“å­˜ç›¸åŒå‰ç¼€çš„ KV Cacheï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

**åº”ç”¨åœºæ™¯ï¼š**
- å¤šè½®å¯¹è¯ï¼ˆsystem prompt ç›¸åŒï¼‰
- Few-shot learningï¼ˆexamples ç›¸åŒï¼‰
- æ‰¹é‡æ¨ç†ï¼ˆå…±åŒå‰ç¼€ï¼‰

**ç¤ºä¾‹ï¼š**

```
è¯·æ±‚ 1: "You are a helpful assistant. User: Hello"
è¯·æ±‚ 2: "You are a helpful assistant. User: Hi"

å…±åŒå‰ç¼€: "You are a helpful assistant."
â”œâ”€ åªè®¡ç®—ä¸€æ¬¡ prefill
â”œâ”€ åç»­è¯·æ±‚å¤ç”¨ KV Cache blocks
â””â”€ èŠ‚çœè®¡ç®—å’Œæ˜¾å­˜
```

### 3. Continuous Batching

**åŸç†ï¼š** åŠ¨æ€æ‰¹å¤„ç†ï¼Œåºåˆ—å®Œæˆåç«‹å³å¤„ç†æ–°è¯·æ±‚ã€‚

**å¯¹æ¯”ï¼š**

```
Static Batchingï¼ˆä¼ ç»Ÿï¼‰:
Batch 1: [Seq A â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ, Seq B â–ˆâ–ˆâ–ˆâ–ˆ, Seq C â–ˆâ–ˆ]  ç­‰å¾…æœ€æ…¢çš„
Batch 2: [Seq D â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ, Seq E â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]         æ‰èƒ½å¼€å§‹

Continuous Batching:
Step 1: [Seq A, Seq B, Seq C]
Step 2: [Seq A, Seq B, Seq C]
Step 3: [Seq A, Seq C, Seq D]  â† Seq B å®Œæˆï¼ŒSeq D åŠ å…¥
Step 4: [Seq A, Seq C, Seq D]
Step 5: [Seq C, Seq D, Seq E]  â† Seq A å®Œæˆï¼ŒSeq E åŠ å…¥
```

**ä¼˜åŠ¿ï¼š** æé«˜ GPU åˆ©ç”¨ç‡ï¼Œé™ä½å¹³å‡å»¶è¿Ÿã€‚

### 4. CUDA Graph

**åŸç†ï¼š** é¢„å…ˆæ•è·è®¡ç®—å›¾ï¼Œå‡å°‘ kernel launch overheadã€‚

**æµç¨‹ï¼š**

```
Warmup é˜¶æ®µ:
for batch_size in [1, 2, 4, 8, 16, ...]:
    1. åˆ›å»ºå‡è¾“å…¥
    2. æ‰§è¡Œä¸€æ¬¡æ¨ç†ï¼ˆwarmupï¼‰
    3. æ•è· CUDA Graph
    4. å­˜å‚¨ graph å’Œè¾“å…¥/è¾“å‡ºç¼“å†²åŒº

Inference é˜¶æ®µ:
1. æ ¹æ®å½“å‰ batch_size é€‰æ‹©å¯¹åº” graph
2. å°†æ•°æ®å¤åˆ¶åˆ° graph çš„è¾“å…¥ç¼“å†²åŒº
3. graph.replay()  â† é‡æ”¾è®¡ç®—å›¾ï¼ˆæå¿«ï¼‰
4. ä»è¾“å‡ºç¼“å†²åŒºè¯»å–ç»“æœ
```

**ä¼˜åŠ¿ï¼š** Decode é˜¶æ®µå¯æé€Ÿ ~1.5xã€‚

### 5. Tensor Parallelism

**åŸç†ï¼š** å°†æ¨¡å‹æƒé‡å’Œè®¡ç®—åˆ‡åˆ†åˆ°å¤šä¸ª GPUã€‚

**åˆ‡åˆ†ç­–ç•¥ï¼š**

```
QKV Linear:
weight: [hidden_size, (num_heads + 2*num_kv_heads) * head_dim]
â†“
GPU 0: weight[:, :hidden_size//2]
GPU 1: weight[:, hidden_size//2:]

Output Linear:
â†“
GPU 0 output â†’ AllReduce â†’ æœ€ç»ˆè¾“å‡º
GPU 1 output â†’ AllReduce â†’ æœ€ç»ˆè¾“å‡º
```

**ä½¿ç”¨ï¼š**

```python
llm = LLM(model_path, tensor_parallel_size=4)  # 4 GPU å¹¶è¡Œ
```

---

## æ€§èƒ½æŒ‡æ ‡

æ ¹æ® README ä¸­çš„ benchmark ç»“æœï¼ˆRTX 4070 Laptopï¼ŒQwen3-0.6Bï¼‰ï¼š

| æŒ‡æ ‡ | vLLM | Nano-vLLM |
|-----|------|-----------|
| è¾“å‡º tokens | 133,966 | 133,966 |
| æ€»æ—¶é—´ | 98.37s | 93.41s |
| ååé‡ | 1361.84 tok/s | **1434.13 tok/s** |

**Nano-vLLM æ¯” vLLM å¿« ~5.3%ï¼**

---

## æ€»ç»“

Nano-vLLM é€šè¿‡ç®€æ´çš„ä»£ç å®ç°äº†ç”Ÿäº§çº§ LLM æ¨ç†å¼•æ“çš„æ ¸å¿ƒåŠŸèƒ½ï¼š

1. **é«˜æ•ˆå†…å­˜ç®¡ç†**ï¼šPagedAttention + Prefix Caching
2. **çµæ´»è°ƒåº¦**ï¼šContinuous Batching + Preemption
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šCUDA Graph + Tensor Parallelism
4. **æ˜“äºç†è§£**ï¼š~1200 è¡Œä»£ç ï¼Œæ¸…æ™°çš„æ¨¡å—åˆ’åˆ†

æ•°æ®æµåŠ¨è·¯å¾„æ¸…æ™°ï¼š

```
ç”¨æˆ·è¾“å…¥ â†’ Token ç¼–ç  â†’ Sequence å¯¹è±¡ â†’ è°ƒåº¦é˜Ÿåˆ—
â†’ Block åˆ†é… â†’ æ¨¡å‹æ¨ç† (Prefill/Decode) â†’ é‡‡æ ·
â†’ åå¤„ç† â†’ å®Œæˆåˆ¤æ–­ â†’ Token è§£ç  â†’ è¿”å›ç»“æœ
```

è¿™ç§è®¾è®¡æ—¢ä¿è¯äº†æ¨ç†æ•ˆç‡ï¼Œåˆä¿æŒäº†ä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚
